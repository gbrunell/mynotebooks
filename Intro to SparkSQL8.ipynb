{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/sparkSQL4.png' width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/sparkSQL2.png' width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/sparkSQL3.png' width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/sparkSQL1.png' width=\"80%\" height=\"80%\"></img>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting started:\n",
    "Create a SQL Context from the Spark Context, sc, which is predefined in every notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9371912d6214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "#sqlContext is used for defining Dataframes and working with SparkSQL\n",
    "#use sc to create our sqlContext, sc has the connection information for the\n",
    "#Spark enviroment\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SQL Context queries Dataframes, not RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data file on world banks will downloaded from GitHub after removing any previous data that may exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In the Data Scientist Workbench (DSWB) you can prefice commands with a ! to run shell commands\n",
    "# here we remove any files with the name of the file we are going to download\n",
    "# then download the file\n",
    "\n",
    "!rm world_bank.json.gz -f\n",
    "!wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#A Dataframe will be created using the sqlContext to read the file. Many other types are supported including text and Parquet\n",
    "\n",
    "Here we are creating a Dataframe, similar to an RDD, but with a schema and abstraction that allows\n",
    "for SQL to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#You can load json, text and other files using sqlContext\n",
    "#unlinke an RDD, this will attempt to create a schema around the data\n",
    "#self describing data works really well for this\n",
    "\n",
    "example1_df = sqlContext.read.json(\"/resources/world_bank.json.gz\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Spark SQL has the ability to infer the schema of JSON data and understand the structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#once we have created the Dataframe, we can print out the schema to see the shape of the data\n",
    "\n",
    "print example1_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Let's take a look at the first two rows of data\n",
    "\n",
    "The example below enumerates our \"take\" command that pulls 2 items from the Dataframe\n",
    "<br>a simpiler option to see the data could also be:<br>\n",
    "\n",
    "print example1_df.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in example1_df.take(2):\n",
    "    print row\n",
    "    print \"*\" * 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Now let's register a table which is a pointer to the Dataframe and allows data access via Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Simply use the Dataframe Object to create the table:\n",
    "example1_df.registerTempTable(\"world_bank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now that the table is registered we can execute sql commands \n",
    "#NOTE that the returned object is another Dataframe:\n",
    "\n",
    "temp_df =  sqlContext.sql(\"select * from world_bank limit 2\")\n",
    "\n",
    "print type(temp_df)\n",
    "print \"*\" * 20\n",
    "print temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#one nice feature of the notebooks and python is that we can show it in a table via Pandas\n",
    "sqlContext.sql(\"select id, borrower from world_bank limit 2\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here is a simple group by example:\n",
    "\n",
    "query = \"\"\"\n",
    "select\n",
    "    regionname ,\n",
    "    count(*) as project_count\n",
    "from world_bank\n",
    "group by regionname \n",
    "order by count(*) desc\n",
    "\"\"\"\n",
    "\n",
    "sqlContext.sql(query).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#subselect works as well:\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "select * from\n",
    "    (select\n",
    "        regionname ,\n",
    "        count(*) as project_count\n",
    "    from world_bank\n",
    "    group by regionname \n",
    "    order by count(*) desc) table_alias\n",
    "limit 2\n",
    "\"\"\"\n",
    "\n",
    "sqlContext.sql(query).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Simple Example of Adding a Schema (headers) to an RDD and using it as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below a simple RDD is created with Random Data in two columns and an ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#first let's create a simple RDD\n",
    "\n",
    "#create a Python list of lists for our example\n",
    "data_e2 = []\n",
    "for x in range(1,6):\n",
    "    random_int = int(random.random() * 10)\n",
    "    data_e2.append([x, random_int, random_int^2])\n",
    "\n",
    "#create the RDD with the random list of lists\n",
    "rdd_example2 = sc.parallelize(data_e2)\n",
    "print rdd_example2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#now we can assign some header information\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"ID VAL1 VAL2\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaExample = sqlContext.createDataFrame(rdd_example2, schema)\n",
    "\n",
    "# Register the DataFrame as a table.\n",
    "schemaExample.registerTempTable(\"example2\")\n",
    "\n",
    "# Pull the data\n",
    "print schemaExample.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In Dataframes we can reference the columns names for example:\n",
    "\n",
    "for row in schemaExample.take(2):\n",
    "    print row.ID, row.VAL1, row.VAL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Again a simple sql example:\n",
    "\n",
    "sqlContext.sql(\"select * from example2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Another Example of creating a Dataframe from an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remember this RDD:\n",
    "print type(rdd_example2)\n",
    "print rdd_example2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we can use Row to specify the name of the columns with a Map, then use that to create the Dataframe\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rdd_example3 = rdd_example2.map(lambda x: Row(id=x[0], val1=x[1], val2=x[2]))\n",
    "\n",
    "print rdd_example3.collect()\n",
    "                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we can convert rdd_example3 to a Dataframe\n",
    "\n",
    "df_example3 = rdd_example3.toDF()\n",
    "df_example3.registerTempTable(\"df_example3\")\n",
    "\n",
    "print type(df_example3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now a simple SQL statement\n",
    "sqlContext.sql(\"select * from df_example3\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Joins are supported, here is a simple example with our two new tables\n",
    "We can join example2 and example3 on ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select\n",
    "    *\n",
    "from\n",
    "    example2 e2\n",
    "inner join df_example3 e3 on\n",
    "    e2.id = e3.id\n",
    "\"\"\"\n",
    "\n",
    "print sqlContext.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Alternatively you can join within Python as well (or Scala of course)\n",
    "\n",
    "df_example4 = df_example3.join(schemaExample, schemaExample[\"id\"] == df_example3[\"ID\"] )\n",
    "\n",
    "for row in df_example4.take(5):\n",
    "    print row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Another powerful feature is the ability to create Functions and Use them in SQL Here is a simple example\n",
    "\n",
    "First we create a function in Python, then register it using sqlContext allowing for us to call it via SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first we create a Python function:\n",
    "\n",
    "def simple_function(v):\n",
    "    return int(v * 10)\n",
    "\n",
    "#test the function\n",
    "print simple_function(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we can register the function for use in SQL\n",
    "sqlContext.registerFunction(\"simple_function\", simple_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we can apply the filter in a SQL Statement\n",
    "query = \"\"\"\n",
    "select\n",
    "    ID,\n",
    "    VAL1,\n",
    "    VAL2,\n",
    "    simple_function(VAL1) as s_VAL1,\n",
    "    simple_function(VAL2) as s_VAL2\n",
    "from\n",
    " example2\n",
    "\"\"\"\n",
    "sqlContext.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#note that the VAL1 and VAL2 look like strings, we can cast them as well\n",
    "query = \"\"\"\n",
    "select\n",
    "    ID,\n",
    "    VAL1,\n",
    "    VAL2,\n",
    "    simple_function(cast(VAL1 as int)) as s_VAL1,\n",
    "    simple_function(cast(VAL2 as int)) as s_VAL2\n",
    "from\n",
    " example2\n",
    "\"\"\"\n",
    "sqlContext.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Pandas Example\n",
    "Pandas is a common abstraction for working with data in Python.\n",
    "\n",
    "We can turn Pandas Dataframes into Spark Dataframes, the advantage of this \n",
    "could be scale or allowing us to run SQL statements agains the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import pandas library\n",
    "import pandas as pd\n",
    "print pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's grab some UFO data to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm SIGHTINGS.csv -f\n",
    "!wget https://www.quandl.com/api/v3/datasets/NUFORC/SIGHTINGS.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using the CSV file from earlier, we can create a Pandas Dataframe:\n",
    "pandas_df = pd.read_csv(\"/resources/SIGHTINGS.csv\")\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now convert to Spark Dataframe\n",
    "spark_df = sqlContext.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#explore the first two rows:\n",
    "\n",
    "for row in spark_df.take(2):\n",
    "    print row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#register the Spark Dataframe as a table\n",
    "spark_df.registerTempTable(\"ufo_sightings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now a SQL statement\n",
    "print sqlContext.sql(\"select * from ufo_sightings limit 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Visualizing the Data\n",
    "Here are some simple ways to create charts using Pandas output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to display in the notebook we need to tell matplotlib to render inline\n",
    "at this point import the supporting libraries as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can call a function \"plot\" to create the charts.\n",
    "Since most charts are created from aggregates the record\n",
    "set should be small enough to store in Pandas\n",
    "\n",
    "We can take our UFO data from before and create a \n",
    "Pandas Dataframe from the Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ufos_df = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot we call the \"plot\" method and specify the type, x and y axis columns\n",
    "and optionally the size of the chart.\n",
    "\n",
    "Many more details can be found here:\n",
    "http://pandas.pydata.org/pandas-docs/stable/visualization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufos_df.plot(kind='bar', x='Reports', y='Count', figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look good, there are too many observations, let's check how many:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sqlContext.sql(\"select count(*) from ufo_sightings limit 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ideally we could just group by year, there are many ways we could solve that:</h2>\n",
    "\n",
    "1) parse the Reports column in SQL and output the year, then group on it\n",
    "2) create a simple Python function to parse the year and call it via sql\n",
    "3) as shown below: use map against the Dataframe and append a new column with \"year\"\n",
    "\n",
    "Tge example below takes the existing data for each row and appends a new column \"year\" \n",
    "by taking the first for characters from the Reports column\n",
    "\n",
    "Reports looks like this for example:\n",
    "2016-01-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufos_df = spark_df.map(lambda x: Row(**dict(x.asDict(), year=int(x.Reports[0:4]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check to verify we get the expected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ufos_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the new Dataframe as a table \"ufo_withyear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ufos_df.registerTempTable(\"ufo_withyear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can group by year, order by year and filter to the last 66 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select \n",
    "    sum(count) as count, \n",
    "    year \n",
    "from ufo_withyear\n",
    "where year > 1950\n",
    "group by year\n",
    "order by year\n",
    "\"\"\"\n",
    "pandas_ufos_withyears = sqlContext.sql(query).toPandas()\n",
    "pandas_ufos_withyears.plot(kind='bar', x='year', y='count', figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!jps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kill -9 1395\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kill -9 936\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
