{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Online Purchase Recommendations\n",
    "\n",
    "Learn how to create a recommendation engine using the Alternating Least Squares algorithm in Spark's machine learning library\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/ALS.png' width=\"70%\" height=\"70%\"></img>\n",
    "\n",
    "###The data\n",
    "\n",
    "This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.  The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Online+Retail\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/FullFile.png' width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Create an RDD from the csv data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Download the data\n",
    "import sys.process._\n",
    "\"rm OnlineRetail.csv.gz -f\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-04-27 19:18:03--  https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 23.235.39.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|23.235.39.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7483128 (7.1M) [application/octet-stream]\n",
      "Saving to: ‘OnlineRetail.csv.gz’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  743K 10s\n",
      "    50K .......... .......... .......... .......... ..........  1% 1.45M 7s\n",
      "   100K .......... .......... .......... .......... ..........  2% 1.46M 6s\n",
      "   150K .......... .......... .......... .......... ..........  2% 97.7M 5s\n",
      "   200K .......... .......... .......... .......... ..........  3% 91.5M 4s\n",
      "   250K .......... .......... .......... .......... ..........  4% 1.43M 4s\n",
      "   300K .......... .......... .......... .......... ..........  4%  132M 3s\n",
      "   350K .......... .......... .......... .......... ..........  5%  139M 3s\n",
      "   400K .......... .......... .......... .......... ..........  6%  113M 3s\n",
      "   450K .......... .......... .......... .......... ..........  6% 1.50M 3s\n",
      "   500K .......... .......... .......... .......... ..........  7%  113M 3s\n",
      "   550K .......... .......... .......... .......... ..........  8%  133M 2s\n",
      "   600K .......... .......... .......... .......... ..........  8%  119M 2s\n",
      "   650K .......... .......... .......... .......... ..........  9% 38.4M 2s\n",
      "   700K .......... .......... .......... .......... .......... 10% 55.1M 2s\n",
      "   750K .......... .......... .......... .......... .......... 10% 1.61M 2s\n",
      "   800K .......... .......... .......... .......... .......... 11% 90.7M 2s\n",
      "   850K .......... .......... .......... .......... .......... 12%  117M 2s\n",
      "   900K .......... .......... .......... .......... .......... 12%  117M 2s\n",
      "   950K .......... .......... .......... .......... .......... 13%  133M 2s\n",
      "  1000K .......... .......... .......... .......... .......... 14%  113M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 15%  135M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 15%  141M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 16%  140M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 17%  110M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 17% 1.63M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 18%  125M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 19%  134M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 19%  121M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 20%  140M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 21% 4.99M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 21%  123M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 22% 48.6M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 23%  142M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 23%  138M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 24%  149M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 25%  131M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 25%  149M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 26%  137M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 27%  141M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 28% 2.67M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 28%  114M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 29%  126M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 30%  129M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 30%  118M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 31% 60.3M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 32%  134M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 32%  140M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 33%  120M 1s\n",
      "  2450K .......... .......... .......... .......... .......... 34%  112M 1s\n",
      "  2500K .......... .......... .......... .......... .......... 34%  119M 1s\n",
      "  2550K .......... .......... .......... .......... .......... 35%  126M 1s\n",
      "  2600K .......... .......... .......... .......... .......... 36%  127M 1s\n",
      "  2650K .......... .......... .......... .......... .......... 36%  132M 1s\n",
      "  2700K .......... .......... .......... .......... .......... 37%  136M 1s\n",
      "  2750K .......... .......... .......... .......... .......... 38%  138M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 38%  122M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 39%  137M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 40%  142M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 41%  138M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 41%  117M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 42% 11.1M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 43%  139M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 43%  139M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 44% 52.2M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 45% 44.6M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 45% 2.65M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 46% 93.6M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 47% 65.7M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 47% 62.2M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 48%  107M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 49%  101M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 49% 96.3M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 50% 54.3M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 51% 95.6M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 51%  123M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 52%  122M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 53%  145M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 54% 75.7M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 54%  114M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 55%  105M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 56% 97.6M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 56%  115M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 57%  130M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 58%  120M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 58%  145M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 59%  140M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 60%  114M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 60%  127M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 61%  131M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 62%  129M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 62%  115M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 63% 80.0M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 64%  129M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 64%  137M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 65%  111M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 66%  130M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 67% 2.44M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 67%  113M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 68% 94.6M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 69%  118M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 69% 95.2M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 70%  132M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 71% 76.6M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 71% 37.5M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 72%  124M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 73%  122M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 73% 93.8M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 74%  122M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 75% 83.3M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 75%  104M 0s\n",
      "  5550K .......... .......... .......... .......... .......... 76% 88.1M 0s\n",
      "  5600K .......... .......... .......... .......... .......... 77% 83.3M 0s\n",
      "  5650K .......... .......... .......... .......... .......... 77%  129M 0s\n",
      "  5700K .......... .......... .......... .......... .......... 78%  121M 0s\n",
      "  5750K .......... .......... .......... .......... .......... 79%  122M 0s\n",
      "  5800K .......... .......... .......... .......... .......... 80%  131M 0s\n",
      "  5850K .......... .......... .......... .......... .......... 80%  104M 0s\n",
      "  5900K .......... .......... .......... .......... .......... 81% 94.0M 0s\n",
      "  5950K .......... .......... .......... .......... .......... 82% 81.3M 0s\n",
      "  6000K .......... .......... .......... .......... .......... 82%  103M 0s\n",
      "  6050K .......... .......... .......... .......... .......... 83%  104M 0s\n",
      "  6100K .......... .......... .......... .......... .......... 84% 98.7M 0s\n",
      "  6150K .......... .......... .......... .......... .......... 84% 14.9M 0s\n",
      "  6200K .......... .......... .......... .......... .......... 85% 73.4M 0s\n",
      "  6250K .......... .......... .......... .......... .......... 86%  121M 0s\n",
      "  6300K .......... .......... .......... .......... .......... 86% 88.6M 0s\n",
      "  6350K .......... .......... .......... .......... .......... 87% 3.14M 0s\n",
      "  6400K .......... .......... .......... .......... .......... 88% 65.7M 0s\n",
      "  6450K .......... .......... .......... .......... .......... 88%  106M 0s\n",
      "  6500K .......... .......... .......... .......... .......... 89% 97.9M 0s\n",
      "  6550K .......... .......... .......... .......... .......... 90% 97.4M 0s\n",
      "  6600K .......... .......... .......... .......... .......... 90%  113M 0s\n",
      "  6650K .......... .......... .......... .......... .......... 91%  115M 0s\n",
      "  6700K .......... .......... .......... .......... .......... 92%  124M 0s\n",
      "  6750K .......... .......... .......... .......... .......... 93%  106M 0s\n",
      "  6800K .......... .......... .......... .......... .......... 93%  125M 0s\n",
      "  6850K .......... .......... .......... .......... .......... 94%  123M 0s\n",
      "  6900K .......... .......... .......... .......... .......... 95%  127M 0s\n",
      "  6950K .......... .......... .......... .......... .......... 95%  114M 0s\n",
      "  7000K .......... .......... .......... .......... .......... 96%  136M 0s\n",
      "  7050K .......... .......... .......... .......... .......... 97%  118M 0s\n",
      "  7100K .......... .......... .......... .......... .......... 97%  132M 0s\n",
      "  7150K .......... .......... .......... .......... .......... 98%  105M 0s\n",
      "  7200K .......... .......... .......... .......... .......... 99%  125M 0s\n",
      "  7250K .......... .......... .......... .......... .......... 99%  124M 0s\n",
      "  7300K .......                                               100%  132M=0.4s\n",
      "\n",
      "2016-04-27 19:18:04 (17.3 MB/s) - ‘OnlineRetail.csv.gz’ saved [7483128/7483128]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys.process._\n",
    "\"wget https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\n",
      "536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/10 8:26,2.55,17850,United Kingdom\n"
     ]
    }
   ],
   "source": [
    "//Put the csv into an RDD (at first, each row in the RDD is a string which\n",
    "//correlates to a line in the csv\n",
    "val loadRetailData = sc.textFile(\"/resources/OnlineRetail.csv.gz\")\n",
    "loadRetailData.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Prepare and shape the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Remove the header from the RDD\n",
    "Type:<br>\n",
    "header = loadRetailData.first()<br>\n",
    "loadRetailData = loadRetailData.filter(lambda line: line != header)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val header = loadRetailData.first()\n",
    "val loadRetailData1 = loadRetailData.filter(line=>line != \"InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\")\n",
    "loadRetailData1.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####To produce the ALS model, we need to train it with each individual purchase.  Each record in the RDD must be the customer id, item id, and the rating.  In this case, the rating is the quantity ordered.  MLlib converts these into a sparce, unfactored matrix.\n",
    "\n",
    "####Split the string in each row by comma\n",
    "Type:<br>\n",
    "loadRetailData = loadRetailData.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val loadRetailDataSplit = loadRetailData1.map(line=>line.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Only keep rows that have a purchase quantity of greater than 0, a customerID not equal to 0, and a non blank stock code after romoving non-numeric characters\n",
    "Type:\n",
    "import re<br>\n",
    "filteredRetailData = loadRetailData.filter(lambda l: int(l[3]) > 0 and len(re.sub(\"\\D\", \"\", l[1])) != 0 and len(l[6]) != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scala.util.matching.Regex\n",
    "val filteredRetailDataFilter = loadRetailDataSplit.filter(l => (((l(3).toInt) > 0) && ((l(6).length) != 0) && ((l(1) forall Character.isDigit))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Only keep the customerID, stock code (with non-numeric characters removed), and quanitity as integers for each row\n",
    "Type:<br>\n",
    "skinnyRetailData = filteredRetailData.map(lambda l: (int(l[6]), int(re.sub(\"\\D\", \"\", l[1])), int(l[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val skinnyRetailData = filteredRetailDataFilter.map(l => (l(6).toInt, l(1).toInt, l(3).toInt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####NOTE:  The original file at UCI's Machine Learning Repository has commas in the product description.  Those have been removed to expediate the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Randomly split the data into a testing set and a training set\n",
    "Type:<br>\n",
    "testRDD, trainRDD = skinnyRetailData.randomSplit([.2,.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72174"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//testRDD, trainRDD = skinnyRetailData.randomSplit([.2,.8])\n",
    "val splits = skinnyRetailData.randomSplit(Array(0.8,0.2))\n",
    "val trainRDD = splits(0)\n",
    "val testRDD = splits(1)\n",
    "trainRDD.count\n",
    "testRDD.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Convert the training RDD into Ratings\n",
    "\n",
    "#####There is no need to do this with the test data becuase the values will be used as is, not as Ratings objects\n",
    "Type:<br>\n",
    "from pyspark.mllib.recommendation import ALS, Rating<br>\n",
    "trainData = trainRDD.map(lambda l:  Rating(l[0],l[1],l[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.recommendation.ALS\n",
    "import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
    "import org.apache.spark.mllib.recommendation.Rating\n",
    "val trainDataRating = trainRDD.map(l => Rating(l._1,l._1,l._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Build the recommendation model\n",
    "\n",
    "###Use trainging RDD to train a model with Alternating Least Squares \n",
    "Type:<br>\n",
    "rank=5#(5 columns in the user-feature and product-feature matricies)<br>\n",
    "iterations=10#(10 factorization runs)<br>\n",
    "model = ALS.train(trainData, rank, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been trained"
     ]
    }
   ],
   "source": [
    "val rank=5 //(5 columns in the user-feature and product-feature matricies)\n",
    "val iterations=10 //(10 factorization runs)\n",
    "val model = ALS.train(trainDataRating, rank, iterations)\n",
    "\n",
    "print(\"The model has been trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this model in 2 ways:\n",
    "\n",
    "1) Use it to predict what the user will rate a certain item (in this case, it is predicting how many of that item a user will buy).  We can use the test sample and compare how many purchases the users made to how many purchases our model predicts they will make.\n",
    "\n",
    "2) Use it to predict items the user will be interested in.  This one is more dificult to quantify, but we can use our intuition to see if the recommended items would be useful to the customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Evaluate the model with the test rdd by using the predictAll function.  The RDD passed to the predictAll function should contain only the customerID and the stock code (use the testing RDD)\n",
    "Type:<br>\n",
    "predict = model.predictAll(testRDD.map(lambda l: (l[0],l[1])))<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Rating(14748,16236,-2603.779325067255), Rating(16940,16008,-21911.322773875094))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predict = model.predict(testRDD.map(l=>(l._1,l._2)))\n",
    "predict.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Join the predict all results with the test RDD.  Map the predict all results and the test RDD to a touple of:<br><br>(the customerID and stock code), and the rating/quantity<br><br>then join them together\n",
    "Type:<br>\n",
    "predictions = predict.map(lambda l: ((l[0],l[1]), l[2]))<br>\n",
    "ratingsAndPredictions = testRDD.map(lambda l: ((l[0], l[1]), l[2])).join(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((14462,16054),(2.0,-5625.242091465685))\n",
      "((17811,16225),(2.0,3034.606979630964))\n"
     ]
    }
   ],
   "source": [
    "val predictions = predict.map { case Rating(user,product,rate)=> ((user,product),rate)}\n",
    "val ratings = testRDD.map { case (user,product,rate) => ((user,product),rate.toDouble)}\n",
    "val ratingsAndPredictions = ratings.join(predictions)\n",
    "ratingsAndPredictions.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Calculate and print the Mean Squared Error.  For all ratings/prediction rows, subtract the actual purchase from the prediction, square the result, and take the mean of all of the squared differences\n",
    "Type:<br>\n",
    "meanSquaredError = ratingsAndPredictions.map(lambda l: (l[1][0] - l[1][1])**2).mean()<br>\n",
    "print 'Mean squared error = %.4f' % meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mMain.scala:43: value - is not a member of (Int, Int)",
      "ratingsAndPredictions.map(l=> ((l._1 - l._1)**2).mean())",
      "                                     ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "//val meanSquaredError = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####This doesn't give us that good of a representation of ranking becuase the ranks are number of purchases.  Something better may be to look at some actual recommendations.\n",
    "Evaluate the model with the recommendProducts function. Pass the recommendProducts funtion a customerID and the number of recommendations you would like to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating(15544,13256,59847.58467112103)\n",
      "Rating(15544,15118,59601.30146039754)\n",
      "Rating(15544,15649,59582.137067303935)\n",
      "Rating(15544,15488,58338.78603892082)\n",
      "Rating(15544,14090,54572.279425788365)\n"
     ]
    }
   ],
   "source": [
    "val recs = model.recommendProducts(15544,5)\n",
    "//print the results (The results should be Ratings objects, \n",
    "//with the given customerID, products, and ratings)\n",
    "//for rec in recs:\n",
    "//    print rec\n",
    "recs.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look up this user and the recommended product ID's in the excel file...\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/user.png' width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This user seems to have purchased a lot of childrens gifts and some holiday items.  The recomendation engine we created suggested some items along these lines\n",
    "\n",
    "#####product=84568:<br>GIRLS ALPHABET IRON ON PATCHES \n",
    "\n",
    "#####product=16033:<br>MINI HIGHLIGHTER PENS\n",
    "\n",
    "#####product=22266:<br>EASTER DECORATION HANGING BUNNY\n",
    "\n",
    "#####product=84598:<br>BOYS ALPHABET IRON ON PATCHES\n",
    "\n",
    "#####product=72803:<br>ROSE SCENT CANDLE JEWELLED DRAWER\n",
    "\n",
    "The ALS algorithm uses some randomness, so the recommendations yours produces may be different than these.\n",
    "\n",
    "###Looking those up in excel is a PAIN!\n",
    "You already know about Spark SQL and dataframes.  Convert this file to a dataframe, register it as a table, and run queries on the recommendations produced and the customer the recommendations are for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Data Citation\n",
    "Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197â€“208, 2012 (Published online before print: 27 August 2012. doi: 10.1057/dbm.2012.17)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10.4 (Spark 1.5.2)",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
