{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hello Spark!!!\n",
    "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n",
    "<img src='https://github.com/carloapp2/SparkPOT/blob/master/spark.png?raw=true' width=\"80%\" height=\"80%\"></img>\n",
    "\n",
    "This notebook will show you some basic concepts to start working with Apache Spark including:\n",
    "\n",
    "- Understanding Spark Context\n",
    "- Creating Resilient Distributed Datasets (RDD)\n",
    "- Performing Data Transformations\n",
    "- Loading Data Files to use with Spark\n",
    "\n",
    "####Tool Tips:\n",
    "- Notice the navigation and command buttons at top of the notebook. Press Play & Stop buttons to execute code and interupt execution.\n",
    "- Notice each cell has type. (Markdown, Code, Etc) This cell is a Markdown cell which is simply HTML informational vs Code cell allows you to execute against spark.\n",
    "- Notice each cell has desigination, for eample In [n]: the number is cell number. When you see In [*]: that means the cell is executing\n",
    "- To see all methonds available for object you can use Tab key Example Enter \"SC.\" press Tab and a drop down will appear.\n",
    "- To execute code in active cell press play button at top or you can use short cut keys Shift-Enter, Ctrl-Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Spark Driver and Workers programs\n",
    "A Spark program has a driver program and a workers program. Worker programs run on cluster nodes or in local threads. RDDs are distributed\u001d",
    " across workers. \n",
    "\n",
    "<img src='https://github.com/carloapp2/SparkPOT/blob/master/Spark%20Architecture.png?raw=true' width=\"80%\" height=\"80%\"></img>\n",
    "\n",
    "###Python Spark (pySpark)\n",
    "We are using the Python programming interface to Spark pySpark. \n",
    "pySpark provides an easy-to-use programming abstraction and parallel runtime.\n",
    "\n",
    "###Spark Context\n",
    "Apache Spark driver application uses a context allow a programming interface to interact with the driver application. This is know as a Spark Context which supports Python, Scala and Java programming languages. The SparkContext object tells Spark how and where to access a cluster.<br>\n",
    "<font color=\"red\">This lab uses IBM's fully managed cloud based notebook enviornment, so the spark context is predefined for you.</font><br>\n",
    "\n",
    "In other enviornments you would need to pick an interprerter (i.e. pyspark for python) and create a Spark Config object to initilize a Spark Context. <br>\n",
    "\n",
    "Example:<br>\n",
    "from pyspark import SparkContext, SparkConf<br>\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)<br>\n",
    "sc = SparkContext(conf=conf)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Execute Spark Context to see if its active in cluster\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Simple App2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.6.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Execute to get the version of the spark driver application\n",
    "\n",
    "#Note: There is different versions of spark which support additional \n",
    "#functionality such as DataFrames, Streaming and Machine Learning.\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Resilient Distributed Datasets\n",
    "\n",
    "Apache Spark uses an abstraction for working with data called RDDs - Resilient Distributed Datasets. An RDD is an immutable fault-tolerant collection of elements that can be operated on in parallel. In Apache Spark all work is expressed by either creating new RDDs, transforming existing RDDs or using RDDs to compute results. When working with RDDs, the Spark Driver application automatically distributes the work accross your cluster.\n",
    "\n",
    "####You can construct RDDs by parallelizing existing Python collections (lists) or by transforming an existing RDDs or from files in HDFS or any other storage system. \n",
    "\n",
    "###Understanding Lazy Evaluations...\n",
    "RDDs have actions, which return values, and transformations, which return pointers to new RDDs. Transformations are transformations that do not initiate execution on the cluster. A transformation is mapped in a Digital Acrylic Graph (DAG) which is used to optimize execution on the cluster which occurs at time of an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an RDD from Python collection of numbers\n",
    "\n",
    "#Create a Python collection\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "#Place the collection into an rdd called x_nbr_rdd using parallelize\n",
    "x_nbr_rdd = sc.parallelize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Notice no return occurs with sc.parallelize()\n",
    "This means sc.parallelize didn't compute a result, so its a transformation. Spark only recorded how to create the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Execute an Action and return the first element\n",
    "x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Notice you get a return on .first()\n",
    "This means .first() is an Action. Spark executed all transformations to compute the result of .first()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Execute a Action and take first 5 elements\n",
    "x_nbr_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Human', 'My Name is Spark']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an RDD from Python collection of strings\n",
    "\n",
    "#Create a Python collection\n",
    "y = [\"Hello Human\", \"My Name is Spark\"]\n",
    "\n",
    "#Place the string value into an rdd called y_str_rdd\n",
    "y_str_rdd = sc.parallelize(y)\n",
    "\n",
    "#Return the first value in yoru RDD - Action\n",
    "y_str_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Transformations\n",
    "As you can see, you created a string \"Hello Human\" and you returned value that was parallelized into RDD first element. If we wanted to work with a corpus of words and run analysis on strings to filter out words, then you would need to map each word into an RDD element.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Some common Transformation Functions\n",
    "\n",
    "- <b>map(func):</b> return a new distributed dataset formed by passing each element of the source through a function func\n",
    "- <b>filter(func):</b> return a new dataset formed by selecting those elements of the source on which func returns true\n",
    "- <b>distinct([numTasks])):</b> return a new dataset that contains the distinct elements of the source dataset\n",
    "- <b>flatMap(func):</b> similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Humman. I'm Apache Spark and I love running analysis on data.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create RDD named Words\n",
    "Words = [\"Hello Humman. I'm Apache Spark and I love running analysis on data.\"]\n",
    "words_rd = sc.parallelize(Words)\n",
    "words_rd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Review: Python lambda Functions\n",
    "\n",
    "- Small anonymous functions (not bound to a name) \n",
    "\n",
    "- <b>lambda a , b : a + b</b> returns the sum of its two arguments\n",
    "\n",
    "- Can use lambda functions wherever function objects are required\n",
    "- Restricted to a single expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Humman.',\n",
       " \"I'm\",\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'and',\n",
       " 'I',\n",
       " 'love',\n",
       " 'running',\n",
       " 'analysis',\n",
       " 'on',\n",
       " 'data.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words_rd2 = words_rd.map(lambda line: line.split(\" \"))\n",
    "Words_rd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Humman.', \"I'm\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform RDD Words into new RDD split on Space character.\n",
    "words_rd2 = words_rd.flatMap(lambda line: line.split(\" \"))\n",
    "words_rd2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Filter Function\n",
    "The filter command creates a new RDD from another RDD based on a filter criteria.\n",
    "\n",
    "filter syntax is .filter(lambda line: \"Filter Criteria Value\" in line) \n",
    "\n",
    "Hint: Use a simple python print command to add string to your spark results and run multiple actions in single cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of words 'Hello'\n",
      "Is: 1\n"
     ]
    }
   ],
   "source": [
    "#Count number of \"Hello\" words\n",
    "\n",
    "#Create a new RDD z_str3_rdd for all \"Hello\" words in corpus of words \n",
    "words_rd3 = words_rd2.filter(lambda line: \"Hello\" in line) \n",
    "\n",
    "#Print count of values in the new RDD which represents number of \"Hello\" words in corpus\n",
    "print \"The count of words \" + repr(words_rd3.first())\n",
    "print \"Is: \" + repr(words_rd3.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Creating an RDD from a data file\n",
    "\n",
    "Apache Spark can access many data sources (Files, HDFS, APIs, Relational Data Sources, Etc.). These files need to be accessable by your Spark cluster.\n",
    "\n",
    "We will use wget to pull Apache Spark README.md file into your fully managed spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: README.md*: No such file or directory\n",
      "rm: -f: No such file or directory\n",
      "--2016-04-19 07:30:47--  https://github.com/carloapp2/SparkPOT/blob/master/README.md\n",
      "Resolving github.com... 192.30.252.121\n",
      "Connecting to github.com|192.30.252.121|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: '/Users/Gbrunell/Documents/SparkNotebooks/README.md'\n",
      "\n",
      "README.md               [ <=>                ]  39.30K   247KB/s    in 0.2s    \n",
      "\n",
      "2016-04-19 07:30:47 (247 KB/s) - '/Users/Gbrunell/Documents/SparkNotebooks/README.md' saved [40242]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm README.md* -f /Users/Gbrunell/Documents/SparkNotebooks/README.md\n",
    "!wget https://github.com/carloapp2/SparkPOT/blob/master/README.md -P /Users/Gbrunell/Documents/SparkNotebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Use SparkContext textFile to convert a text file to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Put Data file into RDD\n",
    "textfile_rdd = sc.textFile(\"/Users/Gbrunell/Documents/SparkNotebooks/README.md\")\n",
    "textfile_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file README.md has the word SPARK in it 49 Times.\n"
     ]
    }
   ],
   "source": [
    "#Create a new RDD for all words \"Spark\" in text file\n",
    "Spark_rdd = textfile_rdd.filter(lambda line: \"Spark\" in line)\n",
    "\n",
    "#Print the count of elements in new RDD\n",
    "print \"The file README.md has the word SPARK in it \" + repr(Spark_rdd.count()) + ' Times.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3424 SparkSubmit\n",
      "3093 SparkSubmit\n",
      "821 \n",
      "2125 \n",
      "3453 Jps\n"
     ]
    }
   ],
   "source": [
    "!jps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: line 0: kill: (795) - No such process\r\n"
     ]
    }
   ],
   "source": [
    "!kill 795\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: line 0: kill: (1311) - No such process\r\n"
     ]
    }
   ],
   "source": [
    "!kill 1311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!jps |grep SparkSubmit |awk '{print $1}'|xargs -n1 -i kill {} j"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}