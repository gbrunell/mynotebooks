{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Statistics\n",
    "<img src='https://raw.githubusercontent.com/bradenrc/Spark_POT/master/Modules/MachineLearning/Classification/titanic.jpg' width=\"70%\" height=\"70%\"></img>\n",
    "With Spark, we can easily describe data and use it to make predictions.  We'll be using the famous Titanic data set from Kaggle (https://www.kaggle.com/c/titanic/data) and the machine learning package in Spark to do just that.\n",
    "## Access your data\n",
    "We have the titanic data on an instance of Object Storage, a cloud datat store for access and storage of unstructured data content.  We'll configure the connection here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_hadoop_config(credentials):\n",
    "    prefix = \"fs.swift.service.\" + credentials['name'] \n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(prefix + \".auth.url\", credentials['auth_url']+'/v3/auth/tokens')\n",
    "    hconf.set(prefix + \".auth.endpoint.prefix\", \"endpoints\")\n",
    "    hconf.set(prefix + \".tenant\", credentials['project_id'])\n",
    "    hconf.set(prefix + \".username\", credentials['user_id'])\n",
    "    hconf.set(prefix + \".password\", credentials['password'])\n",
    "    hconf.setInt(prefix + \".http.port\", 8080)\n",
    "    hconf.set(prefix + \".region\", credentials['region'])\n",
    "    hconf.setBoolean(prefix + \".public\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credentials = {\n",
    "  'auth_url':'https://identity.open.softlayer.com',\n",
    "  'project':'object_storage_5a6ce20f_2d5d_4ce5_afa1_a28eb274ef0f',\n",
    "  'project_id':'44aa87e0d8d8484c9b875270726c0598',\n",
    "  'region':'dallas',\n",
    "  'user_id':'e8e24e0512324d3cadfd9c2539afd277',\n",
    "  'domain_id':'1bb51fba380c4a4e8c5621851fd06eed',\n",
    "  'domain_name':'853513',\n",
    "  'username':'Admin_f78408b189a87fe9e90b67b40d92cc515ff458c0',\n",
    "  'password':\"\"\"y-m6Tl_fHDTuu2D.\"\"\",\n",
    "  'filename':'train.csv',\n",
    "  'container':'notebooks',\n",
    "  'tenantId':'saa8-5843b0fd5f79d7-67415d73dbb5'\n",
    "}\n",
    "\n",
    "credentials['name'] = 'keystone'\n",
    "set_hadoop_config(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "Once we have the data, all of the processing is done in memory.  Here, we're formatting the data, removing columns, dropping rows with insufficient data, creating a DataFrame, and creating columns using user defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712\n",
      "+----+---------+-------+------+-------+--------+---------+----------+-----------+----+------+\n",
      "| age|classRank|   fare|parChi|sibSpou|survived|cherbourg|queenstown|southampton|male|female|\n",
      "+----+---------+-------+------+-------+--------+---------+----------+-----------+----+------+\n",
      "|22.0|        3|   7.25|     0|      1|       0|        0|         0|          1|   1|     0|\n",
      "|38.0|        1|71.2833|     0|      1|       1|        1|         0|          0|   0|     1|\n",
      "+----+---------+-------+------+-------+--------+---------+----------+-----------+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext,Row\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "loadTitanicData = sc.textFile(\"swift://\" + credentials['container'] +\".\" + credentials['name'] + '/' + credentials['filename'])\n",
    "header = loadTitanicData.first()\n",
    "loadTitanicData = loadTitanicData.filter(lambda l: l != header).\\\n",
    "                                map(lambda l: l.split(\",\")).\\\n",
    "                                map(lambda l: [l[1],l[2],l[4],l[5],l[6],l[7],l[9],l[11]]).\\\n",
    "                                filter(lambda l: len(l[3]) > 0 and len(l[7]) > 0)\n",
    "\n",
    "print loadTitanicData.count()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "loadTitanicData = loadTitanicData.map(lambda l: Row(survived=int(l[0]),\\\n",
    "                                    classRank=int(l[1]),\\\n",
    "                                    sex=l[2],\\\n",
    "                                    age=float(l[3]),\\\n",
    "                                    sibSpou=int(l[4]),\\\n",
    "                                    parChi=int(l[5]),\\\n",
    "                                    fare=float(l[6]),\\\n",
    "                                    embarked=l[7]))\n",
    "                                    #cherbourg=0,\\\n",
    "                                    #queenstown=0,\\\n",
    "                                    #southampton=0,\\\n",
    "                                    #male=0,\\\n",
    "                                    #female=0))\n",
    "titanicDf = sqlContext.createDataFrame(loadTitanicData)\n",
    "\n",
    "#titanicDf = titanicDf.map(lambda l: l.cherbourg = 1 if l.embarked == 'C' else l.cherbourg = 0)\n",
    "\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import IntegerType\n",
    "isCherb = UserDefinedFunction(lambda x: 1 if x == 'C' else 0, IntegerType())\n",
    "isQueen = UserDefinedFunction(lambda x: 1 if x == 'Q' else 0, IntegerType())\n",
    "isSouth = UserDefinedFunction(lambda x: 1 if x == 'S' else 0, IntegerType())\n",
    "isMale = UserDefinedFunction(lambda x: 1 if x == 'male' else 0, IntegerType())\n",
    "isFemale = UserDefinedFunction(lambda x: 1 if x == 'female' else 0, IntegerType())\n",
    "titanicDf = titanicDf.withColumn(\"cherbourg\",isCherb(titanicDf.embarked)).\\\n",
    "                    withColumn(\"queenstown\",isQueen(titanicDf.embarked)).\\\n",
    "                    withColumn(\"southampton\",isSouth(titanicDf.embarked)).\\\n",
    "                    withColumn(\"male\",isMale(titanicDf.sex)).\\\n",
    "                    withColumn(\"female\",isFemale(titanicDf.sex))\n",
    "\n",
    "titanicDf = titanicDf.drop(\"sex\").drop(\"embarked\")\n",
    "titanicDf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaining insight\n",
    "### Pearson Correlation\n",
    "Now that our data is formatted, we can start to do some basic statistics.  Let's look at what features correlate with surviving the titanic crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age -0.0824458680434\n",
      "classRank -0.356461588445\n",
      "fare 0.266099600477\n",
      "parChi 0.0952652942869\n",
      "sibSpou -0.0155230236317\n",
      "survived 1.0\n",
      "cherbourg 0.195672717021\n",
      "queenstown -0.0489660937057\n",
      "southampton -0.159015410677\n",
      "male -0.536761623349\n",
      "female 0.536761623349\n"
     ]
    }
   ],
   "source": [
    "for col in titanicDf.columns:\n",
    "    print col + \" \" + str(titanicDf.corr('survived',col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look like age had as much impact as we would have guessed.  Let's try to find to correlation after accounting for gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male age -0.119617523233\n",
      "\n",
      "female age 0.110711430946\n"
     ]
    }
   ],
   "source": [
    "maleTitanicDf = titanicDf.filter(titanicDf.male == 1)\n",
    "print \"male age \" + str(maleTitanicDf.corr('survived','age'))\n",
    "femaleTitanicDf = titanicDf.filter(titanicDf.female == 1)\n",
    "print \"female age \" + str(femaleTitanicDf.corr('survived','age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Squared Hypothesis Testing\n",
    "Now that we've seen the correlation, we can double check if they are statistically significant with a chi-squared test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 2 \n",
      "statistic = 91.08074548791019 \n",
      "pValue = 0.0 \n",
      "Very strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 87 \n",
      "statistic = 104.09890718065553 \n",
      "pValue = 0.10209429637643497 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 5 \n",
      "statistic = 22.454253153661288 \n",
      "pValue = 4.2907488830223883E-4 \n",
      "Very strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 6 \n",
      "statistic = 28.78494064715035 \n",
      "pValue = 6.681060065050204E-5 \n",
      "Very strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 218 \n",
      "statistic = 348.2907954802173 \n",
      "pValue = 4.748476645222155E-8 \n",
      "Very strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 1 \n",
      "statistic = 27.260922276697414 \n",
      "pValue = 1.7776808480807205E-7 \n",
      "Very strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 1 \n",
      "statistic = 1.707146972952996 \n",
      "pValue = 0.19135595497570845 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 1 \n",
      "statistic = 18.0035613929655 \n",
      "pValue = 2.2049207911267743E-5 \n",
      "Very strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 1 \n",
      "statistic = 205.1364846934008 \n",
      "pValue = 0.0 \n",
      "Very strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 1 \n",
      "statistic = 205.1364846934008 \n",
      "pValue = 0.0 \n",
      "Very strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint \n",
    "\n",
    "labRDD = titanicDf.map(lambda l: LabeledPoint(l.survived, [l.classRank,l.age,l.sibSpou,l.parChi,l.fare,\\\n",
    "                                                           l.cherbourg,l.queenstown,l.southampton,l.male,l.female]))\n",
    "goodnessOfFitTestResult = Statistics.chiSqTest(labRDD)\n",
    "for result in goodnessOfFitTestResult:\n",
    "    print result\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Machine Learning\n",
    "We can use observed data to make predictions on guests' survival.  First, we form our data into a usable format, split it into a training set and a test set, and finally, create predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(features=DenseVector([22.0, 3.0, 7.25, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]), label=0.0), Row(features=DenseVector([26.0, 3.0, 7.925, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), label=1.0)]\n",
      "\n",
      "[Row(features=DenseVector([38.0, 1.0, 71.2833, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]), label=1.0), Row(features=DenseVector([35.0, 1.0, 53.1, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]), label=1.0)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "titanicDf = titanicDf.map(lambda l: Row(label=float(l.survived),features=\\\n",
    "                                       Vectors.dense([l.age,float(l.classRank),l.fare,float(l.parChi),float(l.sibSpou),\\\n",
    "                                       float(l.cherbourg),float(l.queenstown),float(l.southampton),\\\n",
    "                                       float(l.male),float(l.female)]))).toDF()\n",
    "testDf, trainDf = titanicDf.randomSplit([.15,.85],1)\n",
    "\n",
    "print testDf.take(2)\n",
    "print\n",
    "print trainDf.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.171171171171\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lrModel = lr.fit(trainDf)\n",
    "\n",
    "lrPred = lrModel.transform(testDf)\n",
    "print lrPred.map(lambda line: (line.label - line.prediction)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.207207207207\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(trainDf)\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(trainDf)\n",
    "\n",
    "dtc = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "dtcPipeline = Pipeline(stages=[labelIndexer, featureIndexer, dtc])\n",
    "\n",
    "dtcModel = dtcPipeline.fit(trainDf)\n",
    "dtcPred = dtcModel.transform(testDf)\n",
    "\n",
    "print dtcPred.map(lambda line: (line.label - line.prediction)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.198198198198\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "rfcPipeline = Pipeline(stages=[labelIndexer, featureIndexer, rfc])\n",
    "\n",
    "rfcModel = rfcPipeline.fit(trainDf)\n",
    "rfcPred = rfcModel.transform(testDf)\n",
    "\n",
    "print rfcPred.map(lambda line: (line.label - line.prediction)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.234234234234\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "gbtPipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "gbtModel = gbtPipeline.fit(trainDf)\n",
    "gbtPred = gbtModel.transform(testDf)\n",
    "\n",
    "print gbtPred.map(lambda line: (line.label - line.prediction)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this case, the logistic regression model was the most accurate.  \n",
    "Finally.... the ultimate test... would I survive the Titanic crash?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[27.0,2.0,50.0,0....|[1.26112758443864...|[0.77922015408913...|       0.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#age,classRank,fare,parChi,sibSpou,cherbourg,queenstown,southampton,male,female\n",
    "userInput = sc.parallelize([Row(features=Vectors.dense([27.0, 2.0, 50.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]))]).toDF()\n",
    "lrModel.transform(userInput).show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}